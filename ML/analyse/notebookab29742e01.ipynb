{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4524050,"sourceType":"datasetVersion","datasetId":2643820}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:49.192669Z","iopub.execute_input":"2025-05-14T10:41:49.193256Z","iopub.status.idle":"2025-05-14T10:41:49.579748Z","shell.execute_reply.started":"2025-05-14T10:41:49.193226Z","shell.execute_reply":"2025-05-14T10:41:49.578729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:50.127777Z","iopub.execute_input":"2025-05-14T10:41:50.128811Z","iopub.status.idle":"2025-05-14T10:41:51.157775Z","shell.execute_reply.started":"2025-05-14T10:41:50.128779Z","shell.execute_reply":"2025-05-14T10:41:51.156684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/jobs-dataset-from-glassdoor/eda_data.csv\")\ndf.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:51.15923Z","iopub.execute_input":"2025-05-14T10:41:51.159758Z","iopub.status.idle":"2025-05-14T10:41:51.322076Z","shell.execute_reply.started":"2025-05-14T10:41:51.159726Z","shell.execute_reply":"2025-05-14T10:41:51.321188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:57.409291Z","iopub.execute_input":"2025-05-14T10:41:57.409604Z","iopub.status.idle":"2025-05-14T10:41:57.435377Z","shell.execute_reply.started":"2025-05-14T10:41:57.409583Z","shell.execute_reply":"2025-05-14T10:41:57.434478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:57.915994Z","iopub.execute_input":"2025-05-14T10:41:57.916395Z","iopub.status.idle":"2025-05-14T10:41:57.965614Z","shell.execute_reply.started":"2025-05-14T10:41:57.916369Z","shell.execute_reply":"2025-05-14T10:41:57.96469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:58.315793Z","iopub.execute_input":"2025-05-14T10:41:58.31608Z","iopub.status.idle":"2025-05-14T10:41:58.325412Z","shell.execute_reply.started":"2025-05-14T10:41:58.31606Z","shell.execute_reply":"2025-05-14T10:41:58.32429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:58.62757Z","iopub.execute_input":"2025-05-14T10:41:58.627859Z","iopub.status.idle":"2025-05-14T10:41:58.645067Z","shell.execute_reply.started":"2025-05-14T10:41:58.627839Z","shell.execute_reply":"2025-05-14T10:41:58.64427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Categorical columns & numerical columns\ncategorical_cols = df.select_dtypes(include='object').columns.to_list()\nnumerical_cols = df.select_dtypes(include=['int64','float64']).columns.to_list()\nprint(categorical_cols)\nprint(numerical_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:58.850071Z","iopub.execute_input":"2025-05-14T10:41:58.850705Z","iopub.status.idle":"2025-05-14T10:41:58.857503Z","shell.execute_reply.started":"2025-05-14T10:41:58.850677Z","shell.execute_reply":"2025-05-14T10:41:58.856615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.drop('avg_salary',axis=1)\ny = df['avg_salary']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:41:59.037906Z","iopub.execute_input":"2025-05-14T10:41:59.038283Z","iopub.status.idle":"2025-05-14T10:41:59.044118Z","shell.execute_reply.started":"2025-05-14T10:41:59.038251Z","shell.execute_reply":"2025-05-14T10:41:59.043167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols_x = X.select_dtypes(include='object').columns.to_list()\nnumerical_cols_x = X.select_dtypes(include=['int64','float64']).columns.to_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:42:09.931825Z","iopub.execute_input":"2025-05-14T10:42:09.932152Z","iopub.status.idle":"2025-05-14T10:42:09.938966Z","shell.execute_reply.started":"2025-05-14T10:42:09.932127Z","shell.execute_reply":"2025-05-14T10:42:09.938146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#encoding & standardization \nfrom sklearn.preprocessing import StandardScaler\n\n# One-hot encoding for categorical columns\nX_encoded = pd.get_dummies(X[categorical_cols_x])\n\n# Standard scaling for numerical columns\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(df[numerical_cols_x]), columns=numerical_cols_x)\n\n# Combine encoded and scaled dataframes\nX_processed = pd.concat([X_encoded, X_scaled], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:45:52.190037Z","iopub.execute_input":"2025-05-14T10:45:52.190381Z","iopub.status.idle":"2025-05-14T10:45:52.229144Z","shell.execute_reply.started":"2025-05-14T10:45:52.190356Z","shell.execute_reply":"2025-05-14T10:45:52.228292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_processed.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:47:08.608169Z","iopub.execute_input":"2025-05-14T10:47:08.608581Z","iopub.status.idle":"2025-05-14T10:47:08.615339Z","shell.execute_reply.started":"2025-05-14T10:47:08.608556Z","shell.execute_reply":"2025-05-14T10:47:08.614319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Initialize PCA - let's keep enough components to explain 95% of variance\npca = PCA(n_components=0.95)\n\n# Fit and transform the data\nX_pca = pd.DataFrame(pca.fit_transform(X_processed))\n\n# Print explained variance ratio and number of components\nprint(f\"Number of components selected: {pca.n_components_}\")\nprint(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")\n\n# Plot explained variance ratio\nplt.figure(figsize=(10,6))\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance Ratio')\nplt.title('Explained Variance vs Number of Components')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:47:16.460517Z","iopub.execute_input":"2025-05-14T10:47:16.460837Z","iopub.status.idle":"2025-05-14T10:47:17.905895Z","shell.execute_reply.started":"2025-05-14T10:47:16.460813Z","shell.execute_reply":"2025-05-14T10:47:17.904911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_pca,y,test_size=0.3,random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:47:26.730281Z","iopub.execute_input":"2025-05-14T10:47:26.731541Z","iopub.status.idle":"2025-05-14T10:47:26.738835Z","shell.execute_reply.started":"2025-05-14T10:47:26.731499Z","shell.execute_reply":"2025-05-14T10:47:26.73795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#implement models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:47:42.725987Z","iopub.execute_input":"2025-05-14T10:47:42.726394Z","iopub.status.idle":"2025-05-14T10:47:43.227964Z","shell.execute_reply.started":"2025-05-14T10:47:42.726368Z","shell.execute_reply":"2025-05-14T10:47:43.226963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize all models with parameters\nlr = LinearRegression()\n\nridge = Ridge(\n    alpha=1.0,\n    random_state=42\n)\n\nlasso = Lasso(\n    alpha=1.0,\n    random_state=42\n)\n\nelastic = ElasticNet(\n    alpha=1.0,\n    l1_ratio=0.5,\n    random_state=42\n)\n\ndt = DecisionTreeRegressor(\n    max_depth=10,\n    min_samples_split=5,\n    random_state=42\n)\n\nxgb = XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=5,\n    random_state=42\n)\n\nknn = KNeighborsRegressor(\n    n_neighbors=5,\n    weights='uniform'\n)\n\nrf = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=5,\n    random_state=42\n)\n\nada = AdaBoostRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:47:50.325323Z","iopub.execute_input":"2025-05-14T10:47:50.325633Z","iopub.status.idle":"2025-05-14T10:47:50.332262Z","shell.execute_reply.started":"2025-05-14T10:47:50.325612Z","shell.execute_reply":"2025-05-14T10:47:50.33129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Dictionary to store model performances\nmodel_performances = {}\n\n# Fit and evaluate each model\nmodels = {\n    'Linear Regression': lr,\n    'Lasso Regression' : lasso,\n    'Ridge Regression' : ridge,\n    'ElasticNet' : elastic,\n    'Decision Tree': dt,\n    'Random Forest': rf,\n    'XGBoost': xgb,\n    'KNN': knn,\n    'AdaBoost': ada\n}\n\nfor name, model in models.items():\n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    model_performances[name] = rmse\n    \n    print(f\"{name} RMSE: {rmse:.2f}\")\n\n# Plot the results\nplt.figure(figsize=(10,6))\nplt.bar(model_performances.keys(), model_performances.values())\nplt.title('Model Performances (RMSE)')\nplt.xticks(rotation=45)\nplt.ylabel('RMSE')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:47:57.395463Z","iopub.execute_input":"2025-05-14T10:47:57.395777Z","iopub.status.idle":"2025-05-14T10:48:09.318655Z","shell.execute_reply.started":"2025-05-14T10:47:57.395755Z","shell.execute_reply":"2025-05-14T10:48:09.317616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\n# Define RMSE scorer\nrmse_scorer = make_scorer(lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)))\n\n# Parameter grids for each model\nparam_grid_lr = {\n    'fit_intercept': [True],\n    'positive': [False]\n}\n\nparam_grid_dt = {\n    'max_depth': [5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nparam_grid_rf = {\n    'n_estimators': [100, 200],  # Reduced from [100, 200, 300]\n    'max_depth': [5, 10],        # Reduced from [5, 10, 15]\n    'min_samples_split': [5, 10]  # Reduced options\n}\n\nparam_grid_xgb = {\n    'n_estimators': [100],       # Fixed value instead of [100, 200]\n    'max_depth': [3, 5],         # Reduced from [3, 5, 7]\n    'learning_rate': [0.1, 0.3]  # Removed smallest learning rate\n}\n\nparam_grid_ada = {\n    'n_estimators': [50, 100],   # Removed 200\n    'learning_rate': [0.1, 0.3], # Removed smallest learning rate\n    'loss': ['linear', 'square'] # Removed exponential\n}\n\n\nparam_grid_knn = {\n    'n_neighbors': [3, 5, 7, 9],\n    'weights': ['uniform', 'distance'],\n    'p': [1, 2]  # Manhattan or Euclidean distance\n}\n\nparam_grid_ridge = {\n    'alpha': [0.1, 1.0, 10.0],\n    'fit_intercept': [True],\n    'positive': [False]\n}\n\nparam_grid_lasso = {\n    'alpha': [0.1, 1.0, 10.0],\n    'fit_intercept': [True],\n    'positive': [False]\n}\n\nparam_grid_elastic = {\n    'alpha': [0.1, 1.0, 10.0],\n    'l1_ratio': [0.2, 0.5, 0.8],\n    'fit_intercept': [True],\n    'positive': [False]\n}\n\n# Create GridSearchCV objects\ngrid_searches = {\n    'Linear Regression': GridSearchCV(lr, param_grid_lr, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'Ridge Regression': GridSearchCV(ridge, param_grid_ridge, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'Lasso Regression': GridSearchCV(lasso, param_grid_lasso, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'ElasticNet': GridSearchCV(elastic, param_grid_elastic, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'Decision Tree': GridSearchCV(dt, param_grid_dt, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'Random Forest': GridSearchCV(rf, param_grid_rf, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'XGBoost': GridSearchCV(xgb, param_grid_xgb, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'KNN': GridSearchCV(knn, param_grid_knn, scoring=rmse_scorer, cv=5, n_jobs=-1),\n    'AdaBoost': GridSearchCV(ada, param_grid_ada, scoring=rmse_scorer, cv=5, n_jobs=-1)\n}\n\n# Fit all models and store results\nbest_params = {}\nbest_scores = {}\n\nfor name, grid_search in grid_searches.items():\n    print(f\"\\nTraining {name}...\")\n    try:\n        grid_search.fit(X_train, y_train)\n    except Exception as e:\n        print(f\"Error with {name}: {str(e)}\")\n        continue\n    best_params[name] = grid_search.best_params_\n    best_scores[name] = -grid_search.best_score_  # Negative because of scorer\n    print(f\"Best parameters: {grid_search.best_params_}\")\n    print(f\"Best RMSE: {-grid_search.best_score_:.2f}\")\n\n# Plot results\nplt.figure(figsize=(10,6))\nplt.bar(best_scores.keys(), best_scores.values())\nplt.title('Model Performances After Hyperparameter Tuning (RMSE)')\nplt.xticks(rotation=45)\nplt.ylabel('RMSE')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:48:13.912544Z","iopub.execute_input":"2025-05-14T10:48:13.912844Z","iopub.status.idle":"2025-05-14T10:50:45.343065Z","shell.execute_reply.started":"2025-05-14T10:48:13.912825Z","shell.execute_reply":"2025-05-14T10:50:45.341858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the best Ridge model from grid search\nbest_ridge = grid_searches['Ridge Regression'].best_estimator_\n\n# Make predictions on test set\ny_pred_ridge = best_ridge.predict(X_test)\n\n# Plot actual vs predicted values\nplt.figure(figsize=(10,6))\nplt.scatter(y_test, y_pred_ridge, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Salary')\nplt.ylabel('Predicted Salary')\nplt.title('Ridge Regression: Actual vs Predicted Salary')\nplt.tight_layout()\nplt.show()\n\n# Print the best parameters and RMSE\nprint(\"Best Parameters:\", grid_searches['Ridge Regression'].best_params_)\nprint(\"RMSE on test set:\", np.sqrt(mean_squared_error(y_test, y_pred_ridge)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:51:01.932965Z","iopub.execute_input":"2025-05-14T10:51:01.933345Z","iopub.status.idle":"2025-05-14T10:51:02.16617Z","shell.execute_reply.started":"2025-05-14T10:51:01.933312Z","shell.execute_reply":"2025-05-14T10:51:02.165222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# First identify feature columns excluding target\nfeature_cols = [col for col in df.columns if col != 'avg_salary']\n\n# Identify categorical and numerical columns\ncategorical_cols = df[feature_cols].select_dtypes(include='object').columns.to_list()\nnumerical_cols = df[feature_cols].select_dtypes(include=['int64','float64']).columns.to_list()\n\n# Create preprocessing steps with identified columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n    ])\n\n# Create the complete pipeline\nridge_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('pca', PCA(n_components=0.95)),\n    ('ridge', Ridge(alpha=best_params['Ridge Regression']['alpha'], \n                   random_state=42))\n])\n\n# Fit pipeline and make predictions\nridge_pipeline.fit(X, y)\nexample_input = X.iloc[0:1].copy()\nprediction = ridge_pipeline.predict(example_input)\n\n# Print column information and results\nprint(\"Features used in pipeline:\")\nprint(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\nprint(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\nprint(\"\\nPrediction Results:\")\nprint(\"-\" * 50)\nprint(f\"Predicted Salary: ${prediction[0]:,.2f}k\")\nprint(f\"Actual Salary: ${y.iloc[0]:,.2f}k\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:51:06.674605Z","iopub.execute_input":"2025-05-14T10:51:06.674925Z","iopub.status.idle":"2025-05-14T10:51:07.42207Z","shell.execute_reply.started":"2025-05-14T10:51:06.674902Z","shell.execute_reply":"2025-05-14T10:51:07.421269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}