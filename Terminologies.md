# Introduction √† l'Intelligence Artificielle

> _Cours magistral pour √©tudiants de 4√®me ann√©e cycle ing√©nieur/Master 1, alternant th√©orie et pratique, con√ßu pour favoriser la compr√©hension et l‚Äôesprit critique._

***

## Table des mati√®res

1. [Pr√©sentation et objectifs](#presentation-et-objectifs)
2. [S√©ance 1 : Histoire et concepts fondamentaux](#seance-1-histoire-concepts)
3. [S√©ance 2 : Apprentissage supervis√©](#seance-2-apprentissage-supervise)
4. [S√©ance 3 : Apprentissage non supervis√©](#seance-3-apprentissage-non-supervise)
5. [S√©ance 4 : R√©seaux de neurones profonds](#seance-4-reseaux-profonds)
6. [S√©ance 5 : IA responsable](#seance-5-ia-responsable)
7. [S√©ance 6 : Projets et perspectives](#seance-6-projets-perspectives)
8. [Bibliographie s√©lective](#bibliographie)

***

```
## Pr√©sentation et objectifs<a name="presentation-et-objectifs"></a>
```

**Ce cours vise √† :**

- D√©finir les concepts majeurs de l'intelligence artificielle (IA)
- D√©mystifier les m√©thodes d'apprentissage automatique
- Illustrer les applications concr√®tes et les enjeux industriels
- Initier √† l'√©thique et √† l'explicabilit√© en IA
- Guider la r√©alisation de mini-projets en Python


### Pr√©requis

- Ma√Ætrise de Python
- Bases de statistiques (moyenne, variance, corr√©lation)

***

```
# S√©ance 1 : Histoire et concepts fondamentaux<a name="seance-1-histoire-concepts"></a>
```

**Objectifs d'apprentissage :**

- Comprendre l'√©volution et le vocabulaire central de l‚ÄôIA
- Saisir les enjeux et typologies

***

### D√©finitions cl√©s

- **Intelligence Artificielle (IA)** : Science et ensemble de techniques visant √† faire r√©aliser √† des machines des t√¢ches consid√©r√©es comme intelligentes (ex : compr√©hension du langage, reconnaissance d‚Äôimage).
- **ANI (Artificial Narrow Intelligence)** : IA sp√©cialis√©e dans une t√¢che pr√©cise (ex : jouer aux √©checs, classifier emails).
- **AGI (Artificial General Intelligence)** : IA g√©n√©raliste, capable de raisonner comme un humain sur divers sujets. AGI n‚Äôexiste que sous forme de concept.


#### Typologie des IA

| Type | D√©finition | Exemple |
| :-- | :-- | :-- |
| ANI | Sp√©cialis√©e, efficace pour une t√¢che. | Siri, traducteur automatique |
| AGI | Question conceptuelle, cognitive g√©n√©rale. | Non-atteint aujourd'hui |

### Bref historique

- **1956** : Terme

IA" introduit, conf√©rence de Dartmouth

- **Ann√©es 1960-1970** : Premiers syst√®mes experts, d√©couverte de l'apprentissage automatique
- **Ann√©es 1980** : Perceptron, d√©but des r√©seaux de neurones
- **2000s-2010s** : Explosion du deep learning

**√âtude de cas** : Evolution de la reconnaissance d‚Äôimage, du simple filtrage √† la d√©tection d‚Äôobjets multi-classes dans les smartphones.

### Enjeux soci√©taux et perspectives

- Impacts sur l‚Äôemploi, la m√©decine, le transport
- Questions √©thiques : confidentialit√©, biais, explicabilit√©

***

#### Quiz d‚Äôouverture (√† r√©aliser)

1. Qu‚Äôest-ce que l‚ÄôANI ?
2. En quelle ann√©e le terme IA a-t-il √©t√© propos√© ?
3. Citez une application industrielle de l‚ÄôIA.

***

#### Extrait Python : Visualiser le jeu de donn√©es MNIST

```python
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
mnist = fetch_openml('mnist_784', version=1)
plt.imshow(mnist.data[0].reshape(28, 28), cmap='gray')  # Affiche un chiffre manuscrit
plt.title(f"Chiffre: {mnist.target[0]}")
plt.show()
```

Ce code illustre une application embl√©matique du deep learning.

***

```
# S√©ance 2 : Apprentissage supervis√©<a name="seance-2-apprentissage-supervise"></a>
```

**Objectifs d'apprentissage :**

- Saisir la diff√©rence r√©gression/classification
- Manipuler des donn√©es tabulaires en Python
- √âvaluer un algorithme avec des m√©triques adapt√©es

***

### Principes fondamentaux

- **Supervis√©** : pr√©sence d'une variable cible ("label")


#### Exemples

- Pr√©dire le prix d‚Äôun appartement (r√©gression)
- D√©tecter des emails spam/non-spam (classification)


### Mod√©lisation

- R√©gression lin√©aire, arbres de d√©cision, k-plus proches voisins (KNN)
- **M√©triques** : MSE (Mean Squared Error) en r√©gression ; pr√©cision (accuracy), rappel (recall) en classification


#### D√©monstration Python : R√©gression lin√©aire

```python
import numpy as np
from sklearn.linear_model import LinearRegression
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])
model = LinearRegression().fit(X, y)
print("Coefficient:", model.coef_)
print("Intercept:", model.intercept_)
```


#### Notion de biais/variance et validation crois√©e

- **Biais** : erreur due √† la simplification excessive du mod√®le
- **Variance** : sensibilit√© aux variations des donn√©es
- **Validation crois√©e** : technique d‚Äô√©valuation robuste, ex : K-fold


#### √âtude de cas : Industrie bancaire

- Pr√©dire le risque de cr√©dit avec des arbres de d√©cision et analyse des biais

***

#### Quiz

1. Quelle diff√©rence entre r√©gression et classification ?
2. Citez une m√©trique de performance pour la classification.
3. Quel est le but de la validation crois√©e ?

***

```
# S√©ance 3 : Apprentissage non supervis√©<a name="seance-3-apprentissage-non-supervise"></a>
```

**Objectifs d'apprentissage :**

- D√©tecter des groupes cach√©s ou des anomalies
- Appliquer clustering et r√©duction de dimension

***

### Clustering : regrouper sans label

- **Algorithmes**: K-means, DBSCAN, hi√©rarchique


#### Cas concret : Segmenter des clients selon leurs achats

#### Code Python simple : K-means

```python
from sklearn.cluster import KMeans
import numpy as np
X = np.array([[1,2],[1,4],[1,0],[4,2],[4,4],[4,0]])
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
print("Labels:", kmeans.labels_)
```


### R√©duction de dimensionnalit√©

- **PCA (Principal Component Analysis)** : compression informative


#### Code Python PCA

```python
from sklearn.decomposition import PCA
X = np.random.rand(100, 5)
pca = PCA(n_components=2)
X_transformed = pca.fit_transform(X)
print(X_transformed.shape)  # (100,2)
```


### D√©tection d'anomalies

- IsoForest, LOF : rep√©rer valeurs atypiques


#### √âtude de cas : D√©tection de fraudes bancaires


***

#### Quiz

1. Qu‚Äôest-ce que le clustering ?
2. √Ä quoi sert la r√©duction de dimensionnalit√© ?
3. Donnez une m√©thode de d√©tection d‚Äôanomalies.

***

```
# S√©ance 4 : R√©seaux de neurones profonds<a name="seance-4-reseaux-profonds"></a>
```

**Objectifs d'apprentissage :**

- Comprendre la structure des r√©seaux de neurones
- Manipuler les notions de couches, fonctions d‚Äôactivation

***

### Perceptron simple

- Mod√©lisation d‚Äôun neurone : somme pond√©r√©e suivie d‚Äôune activation


#### Illustration Python

```python
import numpy as np
def perceptron(X, y, lr=0.1, epochs=10):
    w = np.zeros(X.shape[1])
    for _ in range(epochs):
        for i in range(len(y)):
            y_pred = np.dot(X[i], w) > 0
            w += lr * (y[i] - y_pred) * X[i]
    return w
# Donn√©es fictives
X = np.array([[2, 3], [1, 5], [2, 8], [9, 6]])
y = np.array([0, 0, 0, 1])
w = perceptron(X, y)
print(w)
```


### R√©tropropagation

- Calcul du gradient pour ajuster les param√®tres


### Architectures avanc√©es

- **CNN (Convolutional Neural Network)** : vision/computing image
- **RNN (Recurrent Neural Network)** : s√©quences (texte, s√©ries temporelles)
- **Introduction aux transformers** : mod√®les s√©quentiels avanc√©s (BERT, GPT)


#### √âtude de cas

- Reconnaissance d‚Äôimage m√©dicale (CNN)
- Analyse de texte automatis√©e (RNN, transformers)

***

#### Quiz

1. Qu‚Äôest-ce qu‚Äôun perceptron ?
2. Donnez une diff√©rence entre CNN et RNN.
3. √Ä quoi servent les transformers ?

***

```
# S√©ance 5 : IA responsable<a name="seance-5-ia-responsable"></a>
```

**Objectifs d'apprentissage :**

- Comprendre les enjeux √©thiques et l√©gislatifs
- Manipuler les outils d‚Äôexplicabilit√©

***

### √âthique et biais

- Sources de biais : donn√©es, algorithmes
- Limites des algorithmes : discrimination, g√©n√©ralisation
- √âthique : responsabilit√©, contr√¥le, respect vie priv√©e


### Explicabilit√© en IA (XAI)

- SHAP, LIME : interpr√©ter r√©sultats


#### Exemple Python : Interpr√©tation avec LIME

```python
from lime.lime_tabular import LimeTabularExplainer
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
X = pd.DataFrame({'A':[1,2,3],'B':[4,5,6]})
y = [0,1,0]
clf = RandomForestClassifier().fit(X, y)
explainer = LimeTabularExplainer(X.values, mode="classification")
exp = explainer.explain_instance(X.values[0], clf.predict_proba)
exp.show_in_notebook()
```


### R√©glementation

- **AI Act** : cadre l√©gal europ√©en pour IA


#### √âtude de cas : Reconnaissance faciale publique

- D√©fi √©thique : consentement, surveillance

***

#### Quiz

1. Citez un type de biais rencontr√© en IA.
2. Quel est le r√¥le du AI Act ?
3. Donnez une m√©thode d‚Äôexplicabilit√©.

***

```
# S√©ance 6 : Projets et perspectives<a name="seance-6-projets-perspectives"></a>
```

**Objectifs d'apprentissage :**

- Concevoir un projet IA de bout en bout
- S‚Äôinitier au d√©ploiement et √† MLOps
- Identifier les avanc√©es de l‚ÄôIA g√©n√©rative

***

### Mini-projet Python

- Exemple : D√©tection d‚Äôanomalies de transactions bancaires


#### √âtapes d‚Äôun projet

1. D√©finir le probl√®me et valider les donn√©es
2. S√©lectionner/entra√Æner le mod√®le
3. √âvaluer et interpr√©ter les r√©sultats
4. D√©ployer (Docker, Streamlit, etc.)
5. Maintenir et monitorer (MLOps)

#### Code de base

```python
# Simple pipeline sklearn
from sklearn.pipeline import Pipeline
from sklearn.ensemble import IsolationForest
clf = Pipeline([
    ("model", IsolationForest())
])
X = np.random.rand(100, 5)
clf.fit(X)
pred = clf.predict(X)
print(pred)
```


### IA g√©n√©rative

- GAN (Generative Adversarial Networks)
- Grandes avanc√©es en images, textes (DALL-E, GPT)


#### √âtude de cas : G√©n√©ration d‚Äôimages synth√©tiques

- Application : sant√©, publicit√©, jeux vid√©o

***

#### Quiz

1. Quelles √©tapes essentielles dans un projet IA ?
2. Qu‚Äôest-ce qu‚Äôune IA g√©n√©rative ?

***

# Bibliographie s√©lective<a name="bibliographie"></a>

### Livres et articles fondateurs

- *Pattern Recognition and Machine Learning* ‚Äî C. Bishop
- *The Elements of Statistical Learning* ‚Äî Hastie, Tibshirani, Friedman
- *Deep Learning* ‚Äî Goodfellow, Bengio, Courville
- Articles majeurs (perceptron, CNN, transformers)


### Ressources en ligne

- Coursera : "Machine Learning" ‚Äî Andrew Ng
- GitHub "Awesome AI" : catalogue d‚Äôoutils et projets Python
- OpenAI : blog, documentation technique

***

## Conseils p√©dagogiques et design

- **D√©construire chaque concept** avec sch√©mas (ex: workflow ML), analogies visuelles
- **Utiliser de vraies donn√©es** pour les TP
- **Favoriser les √©changes critiques** : d√©bats, questions ouvertes en quiz

> _√Ä chaque s√©ance : penser alternance th√©orie/pratique, enjeux r√©els, quiz interactif, et ouverture sur l‚Äôindustrie/la recherche._






# D√©finitions Compl√®tes des Termes de la Figure

---

## üî∑ DOMAINES PRINCIPAUX

### **Artificial Intelligence (Intelligence Artificielle)**
> Discipline scientifique visant √† cr√©er des syst√®mes capables d'effectuer des t√¢ches n√©cessitant normalement l'intelligence humaine : raisonnement, apprentissage, perception, compr√©hension du langage et prise de d√©cision.

### **Machine Learning (Apprentissage Automatique)**
> Sous-domaine de l'IA permettant aux syst√®mes d'apprendre √† partir de donn√©es et d'am√©liorer leurs performances sans √™tre explicitement programm√©s pour chaque t√¢che sp√©cifique.

### **Neural Networks/Deep Learning (R√©seaux de Neurones/Apprentissage Profond)**
> Technique de ML utilisant des r√©seaux de neurones artificiels √† multiples couches pour apprendre des repr√©sentations hi√©rarchiques complexes des donn√©es.

### **Data Science (Science des Donn√©es)**
> Domaine interdisciplinaire combinant statistiques, informatique et expertise m√©tier pour extraire des connaissances et insights √† partir de donn√©es structur√©es et non structur√©es.

### **Big Data (M√©gadonn√©es)**
> Ensembles de donn√©es extr√™mement volumineux, complexes et vari√©s qui n√©cessitent des technologies sp√©cialis√©es pour leur stockage, traitement et analyse (caract√©ris√©s par les 5V : Volume, V√©locit√©, Vari√©t√©, V√©racit√©, Valeur).

---

## üìä SUPERVISED LEARNING (Apprentissage Supervis√©)

### **Classification/Regression**
> **Classification** : T√¢che de pr√©diction d'une cat√©gorie discr√®te (ex: spam/non-spam).  
> **Regression** : T√¢che de pr√©diction d'une valeur continue (ex: prix immobilier).

### **Linear Regression (R√©gression Lin√©aire)**
> Mod√®le qui √©tablit une relation lin√©aire entre variables d'entr√©e et sortie : y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô. Utilis√© pour pr√©dire des valeurs continues.

### **Logistic Regression (R√©gression Logistique)**
> Algorithme de classification binaire utilisant la fonction sigmo√Øde pour pr√©dire des probabilit√©s d'appartenance √† une classe (0 ou 1).

### **Linear Neural Network (R√©seau de Neurones Lin√©aire)**
> R√©seau de neurones simple √† une ou plusieurs couches sans fonctions d'activation non-lin√©aires, √©quivalent √† une r√©gression lin√©aire multiple.

### **Naive Bayes**
> Classificateur probabiliste bas√© sur le th√©or√®me de Bayes avec l'hypoth√®se "na√Øve" d'ind√©pendance conditionnelle entre les features. Tr√®s efficace pour la classification de texte.

### **K-Nearest Neighbors (K Plus Proches Voisins)**
> Algorithme qui classe un point en fonction de la classe majoritaire de ses K voisins les plus proches dans l'espace des features. Non param√©trique et bas√© sur la distance.

### **Decision Trees (Arbres de D√©cision)**
> Structure arborescente o√π chaque n≈ìud interne repr√©sente un test sur un attribut, chaque branche le r√©sultat du test, et chaque feuille une d√©cision finale (classe ou valeur).

### **Random Forest (For√™ts Al√©atoires)**
> Ensemble d'arbres de d√©cision entra√Æn√©s sur des sous-ensembles al√©atoires des donn√©es (bagging). Les pr√©dictions sont agr√©g√©es par vote majoritaire (classification) ou moyenne (r√©gression).

### **Support Vector Machines (Machines √† Vecteurs de Support)**
> Algorithme qui cherche l'hyperplan optimal s√©parant les classes avec la marge maximale. Utilise le "kernel trick" pour g√©rer les probl√®mes non-lin√©aires.

---

## üîç UNSUPERVISED LEARNING (Apprentissage Non-Supervis√©)

### **Dimensionality Reduction (R√©duction de Dimensionnalit√©)**
> Techniques visant √† r√©duire le nombre de variables (features) tout en pr√©servant l'information essentielle. Utilis√© pour visualisation, compression et √©limination du bruit.

### **PCA (Principal Component Analysis - Analyse en Composantes Principales)**
> M√©thode de r√©duction de dimensionnalit√© qui transforme les donn√©es en un nouvel espace orthogonal o√π les axes (composantes principales) capturent la variance maximale.

### **Manifold Learning (Apprentissage de Vari√©t√©s)**
> Ensemble de techniques (t-SNE, UMAP, Isomap) qui d√©couvrent la structure g√©om√©trique sous-jacente de donn√©es haute dimension en les projetant sur une vari√©t√© de dimension inf√©rieure.

### **Clustering (Partitionnement)**
> Regroupement automatique de donn√©es similaires en clusters (groupes) sans labels pr√©existants. Utilis√© pour segmentation, d√©tection de patterns et organisation de donn√©es.

### **K-Means**
> Algorithme de clustering qui partitionne les donn√©es en K groupes en minimisant la variance intra-cluster. Assigne chaque point au centro√Øde le plus proche it√©rativement.

### **Hierarchical Clustering (Clustering Hi√©rarchique)**
> M√©thode qui construit une hi√©rarchie de clusters sous forme d'arbre (dendrogramme) par agr√©gation successive (bottom-up) ou division (top-down) des groupes.

---

## üß† DEEP LEARNING ARCHITECTURES

### **Deep Neural Network (DNN - R√©seau de Neurones Profond)**
> R√©seau de neurones artificiels avec plusieurs couches cach√©es entre l'entr√©e et la sortie. Chaque couche apprend des repr√©sentations de plus en plus abstraites des donn√©es.

### **Convolutional Neural Network (CNN - R√©seau de Neurones Convolutif)**
> Architecture sp√©cialis√©e pour traiter des donn√©es structur√©es en grille (images). Utilise des convolutions pour d√©tecter des features locales (contours, textures, objets) de mani√®re hi√©rarchique.

### **Recurrent Neural Network (RNN - R√©seau de Neurones R√©current)**
> Architecture con√ßue pour traiter des s√©quences (texte, s√©ries temporelles, audio) en maintenant une m√©moire interne. Les connexions forment des cycles permettant de capturer des d√©pendances temporelles.

### **Autoencoder**
> R√©seau de neurones non-supervis√© compos√© d'un encodeur (compression) et d'un d√©codeur (reconstruction). Apprend des repr√©sentations compactes des donn√©es, utilis√© pour r√©duction de dimensionnalit√©, d√©bruitage et g√©n√©ration.

---

## üìà VISUALISATION DU R√âSEAU DE NEURONES (Centre)

La figure centrale montre :

- **Couche d'entr√©e (jaune)** : Re√ßoit les donn√©es brutes (features)
- **Couches cach√©es (rouge/orange)** : Transformations non-lin√©aires successives, extraction de features hi√©rarchiques
- **Couche de sortie (bleu-vert)** : Pr√©diction finale (classe ou valeur)
- **Connexions** : Poids synaptiques ajust√©s durant l'apprentissage par r√©tropropagation

---

## üîó RELATIONS ENTRE LES DOMAINES

1. **IA ‚äÉ ML ‚äÉ DL** : Inclusion hi√©rarchique (du plus g√©n√©ral au plus sp√©cifique)
2. **Data Science ‚à© AI** : La Data Science utilise les outils d'IA/ML pour analyser les donn√©es
3. **Big Data ‚Üí ML** : Le Big Data fournit les donn√©es massives n√©cessaires pour entra√Æner les mod√®les de ML
4. **Supervised ‚à™ Unsupervised = ML** : Les deux paradigmes couvrent l'essentiel du Machine Learning classique
5. **Deep Learning ‚äÇ ML** : Le DL est une technique sp√©cialis√©e du ML bas√©e sur les r√©seaux de neurones profonds

---

## üìö TABLEAU R√âCAPITULATIF

| Domaine | Type | Complexit√© | Cas d'usage typique |
|---------|------|------------|---------------------|
| Linear Regression | Supervis√© | ‚≠ê | Pr√©diction de prix |
| Logistic Regression | Supervis√© | ‚≠ê‚≠ê | Classification binaire |
| Decision Trees | Supervis√© | ‚≠ê‚≠ê | D√©cisions explicables |
| Random Forest | Supervis√© | ‚≠ê‚≠ê‚≠ê | Classification/R√©gression robuste |
| SVM | Supervis√© | ‚≠ê‚≠ê‚≠ê | Classification haute dimension |
| K-Means | Non-supervis√© | ‚≠ê‚≠ê | Segmentation client |
| PCA | Non-supervis√© | ‚≠ê‚≠ê | R√©duction de dimension |
| DNN | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | Classification complexe |
| CNN | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | Vision par ordinateur |
| RNN | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | Traitement de s√©quences |
| Autoencoder | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | G√©n√©ration, compression |

---

Cette figure constitue une **carte conceptuelle compl√®te** de l'√©cosyst√®me moderne de l'Intelligence Artificielle et de la Data Science, montrant clairement les relations, hi√©rarchies et applications de chaque technique. üéØ

---

## üí° NOTES COMPL√âMENTAIRES

### Quand utiliser quoi ?

- **Donn√©es √©tiquet√©es disponibles** ‚Üí Supervised Learning
- **Pas de labels, recherche de patterns** ‚Üí Unsupervised Learning
- **Images, vision** ‚Üí CNN
- **Texte, s√©quences temporelles** ‚Üí RNN/Transformers
- **Interpr√©tabilit√© importante** ‚Üí Decision Trees, Linear Models
- **Performance maximale** ‚Üí Deep Learning, Random Forest
- **Peu de donn√©es** ‚Üí Algorithmes simples (Linear, K-NN)
- **Beaucoup de donn√©es** ‚Üí Deep Learning

### √âvolution historique

1. **1950s-1980s** : IA symbolique, premiers algorithmes (Perceptron)
2. **1990s-2000s** : ML classique (SVM, Random Forest, PCA)
3. **2010s** : R√©volution Deep Learning (AlexNet 2012)
4. **2020s** : Transformers, IA g√©n√©rative, LLMs

---

**Auteur** : Support de cours Data Science  
**Date** : 2025  
**Version** : 1.0
