# D√©finitions Compl√®tes des Termes de la Figure

---

## üî∑ DOMAINES PRINCIPAUX

### **Artificial Intelligence (Intelligence Artificielle)**
> Discipline scientifique visant √† cr√©er des syst√®mes capables d'effectuer des t√¢ches n√©cessitant normalement l'intelligence humaine : raisonnement, apprentissage, perception, compr√©hension du langage et prise de d√©cision.

### **Machine Learning (Apprentissage Automatique)**
> Sous-domaine de l'IA permettant aux syst√®mes d'apprendre √† partir de donn√©es et d'am√©liorer leurs performances sans √™tre explicitement programm√©s pour chaque t√¢che sp√©cifique.

### **Neural Networks/Deep Learning (R√©seaux de Neurones/Apprentissage Profond)**
> Technique de ML utilisant des r√©seaux de neurones artificiels √† multiples couches pour apprendre des repr√©sentations hi√©rarchiques complexes des donn√©es.

### **Data Science (Science des Donn√©es)**
> Domaine interdisciplinaire combinant statistiques, informatique et expertise m√©tier pour extraire des connaissances et insights √† partir de donn√©es structur√©es et non structur√©es.

### **Big Data (M√©gadonn√©es)**
> Ensembles de donn√©es extr√™mement volumineux, complexes et vari√©s qui n√©cessitent des technologies sp√©cialis√©es pour leur stockage, traitement et analyse (caract√©ris√©s par les 5V : Volume, V√©locit√©, Vari√©t√©, V√©racit√©, Valeur).

---

## üìä SUPERVISED LEARNING (Apprentissage Supervis√©)

### **Classification/Regression**
> **Classification** : T√¢che de pr√©diction d'une cat√©gorie discr√®te (ex: spam/non-spam).  
> **Regression** : T√¢che de pr√©diction d'une valeur continue (ex: prix immobilier).

### **Linear Regression (R√©gression Lin√©aire)**
> Mod√®le qui √©tablit une relation lin√©aire entre variables d'entr√©e et sortie : y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô. Utilis√© pour pr√©dire des valeurs continues.

### **Logistic Regression (R√©gression Logistique)**
> Algorithme de classification binaire utilisant la fonction sigmo√Øde pour pr√©dire des probabilit√©s d'appartenance √† une classe (0 ou 1).

### **Linear Neural Network (R√©seau de Neurones Lin√©aire)**
> R√©seau de neurones simple √† une ou plusieurs couches sans fonctions d'activation non-lin√©aires, √©quivalent √† une r√©gression lin√©aire multiple.

### **Naive Bayes**
> Classificateur probabiliste bas√© sur le th√©or√®me de Bayes avec l'hypoth√®se "na√Øve" d'ind√©pendance conditionnelle entre les features. Tr√®s efficace pour la classification de texte.

### **K-Nearest Neighbors (K Plus Proches Voisins)**
> Algorithme qui classe un point en fonction de la classe majoritaire de ses K voisins les plus proches dans l'espace des features. Non param√©trique et bas√© sur la distance.

### **Decision Trees (Arbres de D√©cision)**
> Structure arborescente o√π chaque n≈ìud interne repr√©sente un test sur un attribut, chaque branche le r√©sultat du test, et chaque feuille une d√©cision finale (classe ou valeur).

### **Random Forest (For√™ts Al√©atoires)**
> Ensemble d'arbres de d√©cision entra√Æn√©s sur des sous-ensembles al√©atoires des donn√©es (bagging). Les pr√©dictions sont agr√©g√©es par vote majoritaire (classification) ou moyenne (r√©gression).

### **Support Vector Machines (Machines √† Vecteurs de Support)**
> Algorithme qui cherche l'hyperplan optimal s√©parant les classes avec la marge maximale. Utilise le "kernel trick" pour g√©rer les probl√®mes non-lin√©aires.

---

## üîç UNSUPERVISED LEARNING (Apprentissage Non-Supervis√©)

### **Dimensionality Reduction (R√©duction de Dimensionnalit√©)**
> Techniques visant √† r√©duire le nombre de variables (features) tout en pr√©servant l'information essentielle. Utilis√© pour visualisation, compression et √©limination du bruit.

### **PCA (Principal Component Analysis - Analyse en Composantes Principales)**
> M√©thode de r√©duction de dimensionnalit√© qui transforme les donn√©es en un nouvel espace orthogonal o√π les axes (composantes principales) capturent la variance maximale.

### **Manifold Learning (Apprentissage de Vari√©t√©s)**
> Ensemble de techniques (t-SNE, UMAP, Isomap) qui d√©couvrent la structure g√©om√©trique sous-jacente de donn√©es haute dimension en les projetant sur une vari√©t√© de dimension inf√©rieure.

### **Clustering (Partitionnement)**
> Regroupement automatique de donn√©es similaires en clusters (groupes) sans labels pr√©existants. Utilis√© pour segmentation, d√©tection de patterns et organisation de donn√©es.

### **K-Means**
> Algorithme de clustering qui partitionne les donn√©es en K groupes en minimisant la variance intra-cluster. Assigne chaque point au centro√Øde le plus proche it√©rativement.

### **Hierarchical Clustering (Clustering Hi√©rarchique)**
> M√©thode qui construit une hi√©rarchie de clusters sous forme d'arbre (dendrogramme) par agr√©gation successive (bottom-up) ou division (top-down) des groupes.

---

## üß† DEEP LEARNING ARCHITECTURES

### **Deep Neural Network (DNN - R√©seau de Neurones Profond)**
> R√©seau de neurones artificiels avec plusieurs couches cach√©es entre l'entr√©e et la sortie. Chaque couche apprend des repr√©sentations de plus en plus abstraites des donn√©es.

### **Convolutional Neural Network (CNN - R√©seau de Neurones Convolutif)**
> Architecture sp√©cialis√©e pour traiter des donn√©es structur√©es en grille (images). Utilise des convolutions pour d√©tecter des features locales (contours, textures, objets) de mani√®re hi√©rarchique.

### **Recurrent Neural Network (RNN - R√©seau de Neurones R√©current)**
> Architecture con√ßue pour traiter des s√©quences (texte, s√©ries temporelles, audio) en maintenant une m√©moire interne. Les connexions forment des cycles permettant de capturer des d√©pendances temporelles.

### **Autoencoder**
> R√©seau de neurones non-supervis√© compos√© d'un encodeur (compression) et d'un d√©codeur (reconstruction). Apprend des repr√©sentations compactes des donn√©es, utilis√© pour r√©duction de dimensionnalit√©, d√©bruitage et g√©n√©ration.

---

## üìà VISUALISATION DU R√âSEAU DE NEURONES (Centre)

La figure centrale montre :

- **Couche d'entr√©e (jaune)** : Re√ßoit les donn√©es brutes (features)
- **Couches cach√©es (rouge/orange)** : Transformations non-lin√©aires successives, extraction de features hi√©rarchiques
- **Couche de sortie (bleu-vert)** : Pr√©diction finale (classe ou valeur)
- **Connexions** : Poids synaptiques ajust√©s durant l'apprentissage par r√©tropropagation

---

## üîó RELATIONS ENTRE LES DOMAINES

1. **IA ‚äÉ ML ‚äÉ DL** : Inclusion hi√©rarchique (du plus g√©n√©ral au plus sp√©cifique)
2. **Data Science ‚à© AI** : La Data Science utilise les outils d'IA/ML pour analyser les donn√©es
3. **Big Data ‚Üí ML** : Le Big Data fournit les donn√©es massives n√©cessaires pour entra√Æner les mod√®les de ML
4. **Supervised ‚à™ Unsupervised = ML** : Les deux paradigmes couvrent l'essentiel du Machine Learning classique
5. **Deep Learning ‚äÇ ML** : Le DL est une technique sp√©cialis√©e du ML bas√©e sur les r√©seaux de neurones profonds

---

## üìö TABLEAU R√âCAPITULATIF

| Domaine | Type | Complexit√© | Cas d'usage typique |
|---------|------|------------|---------------------|
| Linear Regression | Supervis√© | ‚≠ê | Pr√©diction de prix |
| Logistic Regression | Supervis√© | ‚≠ê‚≠ê | Classification binaire |
| Decision Trees | Supervis√© | ‚≠ê‚≠ê | D√©cisions explicables |
| Random Forest | Supervis√© | ‚≠ê‚≠ê‚≠ê | Classification/R√©gression robuste |
| SVM | Supervis√© | ‚≠ê‚≠ê‚≠ê | Classification haute dimension |
| K-Means | Non-supervis√© | ‚≠ê‚≠ê | Segmentation client |
| PCA | Non-supervis√© | ‚≠ê‚≠ê | R√©duction de dimension |
| DNN | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | Classification complexe |
| CNN | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | Vision par ordinateur |
| RNN | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | Traitement de s√©quences |
| Autoencoder | Deep Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | G√©n√©ration, compression |

---

Cette figure constitue une **carte conceptuelle compl√®te** de l'√©cosyst√®me moderne de l'Intelligence Artificielle et de la Data Science, montrant clairement les relations, hi√©rarchies et applications de chaque technique. üéØ

---

## üí° NOTES COMPL√âMENTAIRES

### Quand utiliser quoi ?

- **Donn√©es √©tiquet√©es disponibles** ‚Üí Supervised Learning
- **Pas de labels, recherche de patterns** ‚Üí Unsupervised Learning
- **Images, vision** ‚Üí CNN
- **Texte, s√©quences temporelles** ‚Üí RNN/Transformers
- **Interpr√©tabilit√© importante** ‚Üí Decision Trees, Linear Models
- **Performance maximale** ‚Üí Deep Learning, Random Forest
- **Peu de donn√©es** ‚Üí Algorithmes simples (Linear, K-NN)
- **Beaucoup de donn√©es** ‚Üí Deep Learning

### √âvolution historique

1. **1950s-1980s** : IA symbolique, premiers algorithmes (Perceptron)
2. **1990s-2000s** : ML classique (SVM, Random Forest, PCA)
3. **2010s** : R√©volution Deep Learning (AlexNet 2012)
4. **2020s** : Transformers, IA g√©n√©rative, LLMs

---

**Auteur** : Support de cours Data Science  
**Date** : 2025  
**Version** : 1.0
